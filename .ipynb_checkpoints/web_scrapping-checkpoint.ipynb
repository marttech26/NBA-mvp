{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d899147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rono\\anaconda3\\envs\\learn-env\\lib\\site-packages\\requests\\__init__.py:89: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12792a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years we want to scrape\n",
    "years = list(range(1991,2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455b43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop that saves html files to mvp folder\n",
    "url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\"\n",
    "\n",
    "for year in years:\n",
    "    url = url_start.format(year)\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"mvp/{}.html\".format(year), \"w+\", encoding= \"utf-8\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17d2d2",
   "metadata": {},
   "source": [
    "### Parsing mvp table with bsp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a48737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f03d2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decompose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-29b461605a19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"over_header\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'decompose'"
     ]
    }
   ],
   "source": [
    "with open(\"mvp/1991.html\", encoding='utf-8') as f:\n",
    "    page = f.read()\n",
    "    \n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "soup.find('tr', class_=\"over_header\").decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00c088",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_table = soup.find_all(id=\"mvp\")[0]\n",
    "#mvp_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d20d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a51158",
   "metadata": {},
   "source": [
    "#### Read HTML into pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ead2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_1991 = pd.read_html(str(mvp_table))[0]\n",
    "mvp_1991.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_1991[\"Year\"] = 1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19131d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvp_1991.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f60f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to create multiple dfs for all years\n",
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"mvp/{}.html\".format(year), encoding='utf-8') as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    soup.find('tr', class_=\"over_header\").decompose()\n",
    "    mvp_table = soup.find_all(id=\"mvp\")[0]\n",
    "    mvp_df = pd.read_html(str(mvp_table))[0]\n",
    "    mvp_df[\"Year\"] = year        # will help us know which year mvp is from\n",
    "    dfs.append(mvp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all these dfs into 1\n",
    "mvps = pd.concat(dfs)\n",
    "\n",
    "mvps.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f65e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps.to_csv('mvps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea8f31",
   "metadata": {},
   "source": [
    "# Now getting player stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c18594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewriting above code to work for players url\n",
    "player_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
    "\n",
    "for year in years:\n",
    "    url = player_stats_url.format(year)\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"player/{}.html\".format(year), \"w+\", encoding='utf-8') as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7e0a2",
   "metadata": {},
   "source": [
    "But if you check in the player folder, the stored htmls have only 17 players, while the original site has over 600 player stats. This is because the site is in java and is causing rendering problems. It assumes my web renders the page in a browser but thats not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7ddc",
   "metadata": {},
   "source": [
    "So trying to find a way to use my web to render java. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc64eda",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\n",
    "    executable_path=\"C:/Users/Rono/Downloads/chrome-win64\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2868485",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"player/{}.html\".format(year), encoding='utf-8') as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    player_table = soup.find_all(id=\"per_game_stats\")[0]\n",
    "    player_df = pd.read_html(str(player_table))[0]\n",
    "    player_df[\"Year\"] = year\n",
    "    dfs.append(player_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45500f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9769df",
   "metadata": {},
   "outputs": [],
   "source": [
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "players.to_csv('players.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1954d",
   "metadata": {},
   "source": [
    "# Division standings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of single year\n",
    "year = 1991\n",
    "url = team_stats_url.format(year)\n",
    "\n",
    "data = requests.get(url)\n",
    "\n",
    "with open(\"team/{}.html\".format(year), \"w+\", encoding='utf-8') as f:\n",
    "    f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdebf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping for all years\n",
    "for year in years:\n",
    "    url = team_stats_url.format(year)\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"team/{}.html\".format(year), \"w+\", encoding='utf-8') as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71add2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"team/{}.html\".format(year), encoding='utf-8') as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    #soup.find('tr', class_=\"thead\").decompose()\n",
    "    e_table = soup.find_all(id=\"divs_standings_E\")\n",
    "    e_df = pd.read_html(str(e_table))[0]\n",
    "    e_df[\"Year\"] = year\n",
    "    e_df[\"Team\"] = e_df[\"Eastern Conference\"]\n",
    "    del e_df[\"Eastern Conference\"]\n",
    "    dfs.append(e_df)\n",
    "    \n",
    "    w_table = soup.find_all(id=\"divs_standings_W\")\n",
    "    w_df = pd.read_html(str(w_table))[0]\n",
    "    w_df[\"Year\"] = year\n",
    "    w_df[\"Team\"] = w_df[\"Western Conference\"]\n",
    "    del w_df[\"Western Conference\"]\n",
    "    dfs.append(w_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdeb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.to_csv(\"teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a36811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
